{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xFhb7BoyR3YX"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "headers = { 'Accept-Encoding': 'gzip, deflate, sdch', 'Accept-Language': 'en-US,en;q=0.8', 'Upgrade-Insecure-Requests': '1',\n",
        "            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Cache-Control': 'max-age=0', 'Connection': 'keep-alive',}\n",
        "\n",
        "def download(st, end):\n",
        "\n",
        "  data = []\n",
        "\n",
        "  for i in tqdm(range(st,end)):\n",
        "\n",
        "    link = 'https://www.goodreads.com/book/show/' + str(i)\n",
        "    r = requests.get(link, headers=headers, allow_redirects=False)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    try:\n",
        "      title           = soup.find('h1').text.strip()\n",
        "    except:\n",
        "      title = np.nan\n",
        "    try:\n",
        "      author_detail  = '|'.join([i.text for i in soup.find('div', class_ = 'ContributorLinksList').find_all('a')])\n",
        "      author_link    = '|'.join([i.get('href') for i in soup.find('div', class_ = 'ContributorLinksList').find_all('a')])\n",
        "    except:\n",
        "      author_detail = np.nan\n",
        "      author_link   = np.nan\n",
        "    try:\n",
        "      rating = soup.find('div', class_ = 'BookPageMetadataSection__ratingStats').find('div', class_ = 'RatingStatistics__rating').text.strip()\n",
        "    except:\n",
        "      rating = np.nan\n",
        "    try:\n",
        "      rat_rev = soup.find('div', class_ = 'RatingStatistics__meta').get('aria-label')\n",
        "    except:\n",
        "      rat_rev = np.nan\n",
        "    try:\n",
        "      desc = soup.find('div', class_ = 'BookPageMetadataSection__description').text.strip()\n",
        "    except:\n",
        "      desc = np.nan\n",
        "    try:\n",
        "      genre = ','.join([i.find('span').text.strip() for i in soup.find('div', class_ = 'BookPageMetadataSection__genres').find_all('span', class_ = 'BookPageMetadataSection__genreButton')])\n",
        "    except:\n",
        "      genre = np.nan\n",
        "    try:\n",
        "      page = [i.text.strip() for i in soup.find('div', class_ = 'FeaturedDetails').find_all('p') if i.get('data-testid') == 'pagesFormat']\n",
        "    except:\n",
        "      page = np.nan\n",
        "    try:\n",
        "      year  = [i.text.strip() for i in soup.find('div', class_ = 'FeaturedDetails').find_all('p') if i.get('data-testid') == 'publicationInfo']\n",
        "    except:\n",
        "      year = np.nan\n",
        "\n",
        "\n",
        "    data.append([i, title, rating, rat_rev, page, year ,author_detail, genre, desc, author_link])\n",
        "\n",
        "  df = pd.DataFrame(data, columns = ['id', 'title', 'rating', 'rat_rev', 'page', 'year' ,'author_detail',\n",
        "                                    'genre', 'desc', 'author_link'])\n",
        "\n",
        "  df.to_csv('data/' + str(st) + '_' + str(end) + '.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = 400000\n",
        "\n",
        "n = 5000\n",
        "pr = 10\n",
        "\n",
        "process = []\n",
        "\n",
        "for i in range(start,start+n*pr,n):\n",
        "  process.append(multiprocessing.Process(target = download, args = ( i , i + n)))\n",
        "\n",
        "for p in process:\n",
        "  p.start()\n",
        "\n",
        "for p in process:\n",
        "  p.join()"
      ],
      "metadata": {
        "id": "tgxBt_aoSSfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "csvs = [csv for csv in os.listdir('data') if '.csv' in csv]\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for csv in tqdm(csvs):\n",
        "    df_ = pd.read_csv('data/' + csv )\n",
        "    df_['category_'] = csv.split('.csv')[0]\n",
        "    df = pd.concat((df,df_))\n",
        "\n",
        "df.to_csv('data.csv', index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbpWHomOWpl8",
        "outputId": "d117a109-bb05-4980-e980-8650c03ed1a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:00<00:00, 16.02it/s]\n"
          ]
        }
      ]
    }
  ]
}